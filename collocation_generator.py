#!/usr/bin/env python3
"""
Anki æ­é…å¡ç‰‡ç”Ÿæˆå™¨ (Oxford Collocation Dictionary)

ä»ç‰›æ´¥è‹±è¯­æ­é…è¯å…¸æå–åŠ¨è¯æ­é…å’Œä»‹è¯æ­é…ï¼Œ
æŒ‰ä¹‰é¡¹ï¼ˆsenseï¼‰ä¸ºå•ä½ç”Ÿæˆ Anki å¡ç‰‡

ä½¿ç”¨å‰:
1. å¯åŠ¨ mdx-server: python mdx_server.py "ç‰›æ´¥æ­é…è¯å…¸ç›®å½•è·¯å¾„"
2. å…³é—­ Anki (ä»¥ä¾¿è¯»å–æ•°æ®åº“)
3. ä¿®æ”¹ä¸‹æ–¹é…ç½®åŒºçš„è·¯å¾„

ä½¿ç”¨æ–¹å¼:
  ä» Anki æ•°æ®åº“è¯»å–: python collocation_generator.py
  ä»å•è¯åˆ—è¡¨è¯»å–:     python collocation_generator.py -w word1 word2 word3
  ä»æ–‡ä»¶è¯»å–:         python collocation_generator.py -f words.txt
"""

import sqlite3
import requests
from bs4 import BeautifulSoup
from pathlib import Path
import time
import re
import argparse
import sys
import logging

# ================== é…ç½®åŒº ==================

ANKI_DB = "./collection.anki2"
MDX_SERVER_URL = "http://localhost:8000"
MDX_DICT_DIR = "../ç‰›æ´¥è‹±è¯­æ­é…è¯å…¸å…¨ç´¢å¼•"
FREQ_DICT_FILE = "eng_dict.txt"
OUTPUT_FILE = "collocation_cards.txt"
SKIPPED_LOG = "skipped_words.log"

# éš¾è¯ç­›é€‰æ¡ä»¶ï¼ˆä» Anki æå–æ—¶ä½¿ç”¨ï¼‰
EASE_THRESHOLD = 2000
LAPSES_THRESHOLD = 2
MAX_WORDS = 100

# ç»•è¿‡ç³»ç»Ÿä»£ç†ï¼Œç›´æ¥è¿æ¥ localhost
NO_PROXY = {"http": None, "https": None}

# è¦ä¿ç•™çš„æ­é…ç±»åˆ«ï¼ˆsl å±æ€§å€¼ï¼‰
KEEP_SL_TYPES = {
    'verbs',        # VERBSï¼ˆç”¨äºå½¢å®¹è¯/å‰¯è¯è¯æ¡ï¼‰
    'verbandhwd',   # VERB + WORD
    'hwdandverb',   # WORD + VERB
    'prep',         # PREPOSITION
}

# ================== æ—¥å¿— ==================

logging.basicConfig(
    filename=SKIPPED_LOG,
    level=logging.INFO,
    format='%(asctime)s - %(message)s',
    encoding='utf-8',
)
logger = logging.getLogger(__name__)

# ================== è¯é¢‘å­—å…¸ ==================

def load_freq_dict(dict_file):
    """åŠ è½½è¯é¢‘å­—å…¸ï¼Œè¿”å› {word_form: rank} æ˜ å°„ï¼ˆæ‰€æœ‰è¯å½¢éƒ½æ˜ å°„åˆ°åŒä¸€è¡Œå·ï¼‰"""
    freq_map = {}
    try:
        with open(dict_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, start=1):
                words = line.strip().split()
                for word in words:
                    word = word.lower()
                    if word not in freq_map:
                        freq_map[word] = line_num
        print(f"âœ… åŠ è½½è¯é¢‘å­—å…¸: {len(freq_map)} ä¸ªè¯å½¢, {line_num} è¡Œ")
    except FileNotFoundError:
        print(f"âš ï¸  æœªæ‰¾åˆ°è¯é¢‘å­—å…¸æ–‡ä»¶: {dict_file}")
    return freq_map

# ================== æ­¥éª¤1: è·å–å•è¯åˆ—è¡¨ ==================

def get_words_from_list(word_list):
    """ä»å•è¯åˆ—è¡¨è·å–å•è¯"""
    results = []
    for word in word_list:
        word = word.strip().lower()
        word = re.sub(r'[^a-zA-Z\s-]', '', word)
        word = re.sub(r'\s+', ' ', word).strip()
        if word and len(word) > 1:
            results.append({'word': word})
    return results


def get_words_from_file(filename):
    """ä»æ–‡ä»¶è¯»å–å•è¯åˆ—è¡¨ï¼ˆæ¯è¡Œä¸€ä¸ªå•è¯ï¼‰"""
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            words = [line.strip() for line in f if line.strip()]
        return get_words_from_list(words)
    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {filename}")
        return []
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return []


def get_difficult_words():
    """ä» Anki æ•°æ®åº“æå–éš¾è¯"""
    anki_path = Path(ANKI_DB)
    if not anki_path.exists():
        print(f"âŒ æ‰¾ä¸åˆ° Anki æ•°æ®åº“: {ANKI_DB}")
        return []

    conn = sqlite3.connect(ANKI_DB)
    query = f"""
    SELECT DISTINCT
        substr(n.flds, 1, instr(n.flds || char(31), char(31)) - 1) as word,
        c.factor as ease,
        c.lapses
    FROM cards c
    JOIN notes n ON c.nid = n.id
    WHERE c.factor < {EASE_THRESHOLD}
      AND c.lapses > {LAPSES_THRESHOLD}
      AND c.type = 2
    ORDER BY c.lapses DESC, c.factor ASC
    LIMIT {MAX_WORDS}
    """

    results = []
    for row in conn.execute(query):
        word = row[0].strip() if row[0] else ""
        word = re.sub(r'<[^>]+>', '', word)
        word = re.sub(r'sound[^\s]*', '', word, flags=re.IGNORECASE)
        word = re.sub(r'[^a-zA-Z\s-]', '', word)
        word = re.sub(r'\s+', ' ', word).strip()
        word = word.split()[0] if word.split() else ""
        if word and len(word) > 1 and word.isalpha():
            results.append({'word': word.lower()})

    conn.close()
    return results


def get_all_dictionary_words(mdx_dir=None):
    """ä» MDX è¯å…¸æ–‡ä»¶æå–æ‰€æœ‰è¯å¤´"""
    mdx_dir = Path(mdx_dir or MDX_DICT_DIR)
    if not mdx_dir.exists():
        print(f"âŒ æ‰¾ä¸åˆ°è¯å…¸ç›®å½•: {mdx_dir}")
        return []

    mdx_files = list(mdx_dir.glob("*.mdx"))
    if not mdx_files:
        print(f"âŒ åœ¨ {mdx_dir} ä¸­æœªæ‰¾åˆ° .mdx æ–‡ä»¶")
        return []

    mdx_file = mdx_files[0]
    print(f"  ğŸ“– è¯»å–è¯å…¸ç´¢å¼•: {mdx_file.name}")

    try:
        sys.path.insert(0, str(Path(__file__).parent / "../anki-vocab-builder/mdx-server"))
        from mdict_query import IndexBuilder
        builder = IndexBuilder(str(mdx_file))
        keys = builder.get_mdx_keys()
    except ImportError:
        print("âŒ æ— æ³•åŠ è½½ mdict_query æ¨¡å—")
        print("  è¯·ç¡®ä¿ ../anki-vocab-builder/mdx-server/ ç›®å½•å­˜åœ¨")
        return []
    except Exception as e:
        print(f"âŒ è¯»å– MDX æ–‡ä»¶å¤±è´¥: {e}")
        return []

    # åªä¿ç•™çº¯è‹±æ–‡å•è¯ï¼ˆå«è¿å­—ç¬¦ï¼‰ï¼Œè¿‡æ»¤æ‰çŸ­è¯­ã€åæŸ¥ç´¢å¼•ã€ä¸­æ–‡ç­‰
    english_words = set()
    for k in keys:
        k = k.strip()
        if k and re.match(r'^[a-zA-Z]+(-[a-zA-Z]+)*$', k):
            english_words.add(k.lower())

    words_sorted = sorted(english_words)
    print(f"  ğŸ“Š è¯å…¸å…±æœ‰ {len(words_sorted)} ä¸ªè‹±æ–‡è¯å¤´")
    return [{'word': w} for w in words_sorted]

# ================== æ­¥éª¤2: æŸ¥è¯¢è¯å…¸ ==================

# å…¨å±€ç›´æ¥æŸ¥è¯¢å™¨ï¼ˆ--all æ¨¡å¼å¤ç”¨ï¼‰
_mdx_builder = None

def _get_mdx_builder(mdx_dir=None):
    """è·å–æˆ–åˆ›å»º MDX IndexBuilder å®ä¾‹"""
    global _mdx_builder
    if _mdx_builder is not None:
        return _mdx_builder

    mdx_dir = Path(mdx_dir or MDX_DICT_DIR)
    mdx_files = list(mdx_dir.glob("*.mdx"))
    if not mdx_files:
        return None

    try:
        sys.path.insert(0, str(Path(__file__).parent / "../anki-vocab-builder/mdx-server"))
        from mdict_query import IndexBuilder
        _mdx_builder = IndexBuilder(str(mdx_files[0]))
        return _mdx_builder
    except Exception:
        return None


def query_mdx_direct(word, mdx_dir=None):
    """ç›´æ¥æŸ¥è¯¢ MDX æ–‡ä»¶ï¼ˆæ— éœ€ MDX-Serverï¼Œé€Ÿåº¦æ›´å¿«ï¼‰"""
    builder = _get_mdx_builder(mdx_dir)
    if builder is None:
        return None
    try:
        content = builder.mdx_lookup(word)
        if content:
            return ''.join(content)
    except Exception:
        pass
    return None


def check_mdx_server():
    """æ£€æµ‹ MDX-Server æ˜¯å¦è¿è¡Œ"""
    try:
        response = requests.get(f"{MDX_SERVER_URL}/test", timeout=15, proxies=NO_PROXY)
        return response.status_code == 200
    except requests.exceptions.ConnectionError:
        return False
    except requests.exceptions.Timeout:
        print("  âš ï¸  æœåŠ¡å™¨å“åº”æ…¢ï¼Œä½†å¯èƒ½ä»åœ¨è¿è¡Œ")
        choice = input("  ç»§ç»­? (y/n): ").strip().lower()
        return choice == 'y'
    except Exception:
        return False


def query_mdx_server(word):
    """é€šè¿‡ MDX-Server æŸ¥è¯¢å•è¯"""
    try:
        url = f"{MDX_SERVER_URL}/{word}"
        response = requests.get(url, timeout=30, proxies=NO_PROXY)
        response.encoding = 'utf-8'
        if response.status_code == 200:
            return response.text
    except requests.exceptions.Timeout:
        print(f"(è¶…æ—¶)", end=" ")
    except Exception:
        pass
    return None

# ================== æ­¥éª¤3: è§£æç‰›æ´¥æ­é…è¯å…¸ HTML ==================

def parse_collocation_html(html_content, word):
    """
    è§£æç‰›æ´¥æ­é…è¯å…¸ HTMLï¼ŒæŒ‰ä¹‰é¡¹æ‹†åˆ†ä¸ºå¤šå¼ å¡ç‰‡
    åªä¿ç•™åŠ¨è¯æ­é…å’Œä»‹è¯æ­é…

    HTML ç»“æ„:
      <entry>
        <h>word</h>
        <head>
          <p-blk><p>noun</p></p-blk>
          <n-num>1</n-num>
          <def>definition <chn>ä¸­æ–‡</chn></def>
        </head>
        <sl-g-blk sl="verbandhwd|prep|...">
          <sl-g-head>VERB + PITCH</sl-g-head>
          <sl-g>
            <sb-g>
              <cl>collocation word <chn>ä¸­æ–‡</chn></cl>
              <x-blk><x>example <chn>ä¸­æ–‡</chn></x></x-blk>
            </sb-g>
          </sl-g>
        </sl-g-blk>
      </entry>
    """
    if not html_content:
        return []

    soup = BeautifulSoup(html_content, 'html.parser')
    entries = soup.find_all('entry')

    if not entries:
        return []

    cards = []

    for entry in entries:
        # æå–è¯å¤´
        h_elem = entry.find('h')
        headword = h_elem.get_text(strip=True) if h_elem else word

        # æå–è¯æ€§
        head = entry.find('head')
        pos = ""
        sense_num = ""
        def_en = ""
        def_cn = ""

        if head:
            p_elem = head.find('p')
            pos = p_elem.get_text(strip=True) if p_elem else ""

            n_num_elem = head.find('n-num')
            sense_num = n_num_elem.get_text(strip=True) if n_num_elem else ""

            def_elem = head.find('def')
            if def_elem:
                chn_elem = def_elem.find('chn')
                def_cn = chn_elem.get_text(strip=True) if chn_elem else ""
                # è‹±æ–‡é‡Šä¹‰ï¼šå»æ‰ä¸­æ–‡éƒ¨åˆ†
                for chn in def_elem.find_all('chn'):
                    chn.decompose()
                for chnsep in def_elem.find_all('chnsep'):
                    chnsep.decompose()
                def_en = def_elem.get_text(strip=True)

        # æå–æ­é…ï¼ˆåªä¿ç•™åŠ¨è¯å’Œä»‹è¯ï¼‰
        collocation_groups = []
        sl_g_blks = entry.find_all('sl-g-blk')

        for blk in sl_g_blks:
            sl_type = blk.get('sl', '')
            if sl_type not in KEEP_SL_TYPES:
                continue

            # ç±»åˆ«æ ‡é¢˜ï¼ˆè§„èŒƒåŒ–ç©ºæ ¼ï¼‰
            head_elem = blk.find('sl-g-head')
            category_title = head_elem.get_text() if head_elem else sl_type.upper()
            category_title = re.sub(r'\s+', ' ', category_title).strip()

            # æå–æ­é…è¯ç»„å’Œä¾‹å¥
            collocation_items = []
            sb_gs = blk.find_all('sb-g')

            for sb_g in sb_gs:
                item = _parse_sb_g(sb_g)
                if item:
                    collocation_items.append(item)

            if collocation_items:
                collocation_groups.append({
                    'category': category_title,
                    'items': collocation_items,
                })

        # å¦‚æœæ²¡æœ‰åŠ¨è¯/ä»‹è¯æ­é…ï¼Œè·³è¿‡è¯¥ä¹‰é¡¹
        if not collocation_groups:
            continue

        cards.append({
            'word': headword,
            'pos': pos,
            'sense_num': sense_num,
            'def_en': def_en,
            'def_cn': def_cn,
            'collocation_groups': collocation_groups,
        })

    return cards


def _parse_sb_g(sb_g):
    """è§£æä¸€ä¸ª <sb-g> å—ï¼Œæå–æ­é…è¯å’Œä¾‹å¥"""
    # æå–æ­é…è¯
    collocations = []
    chn_text = ""

    for cl in sb_g.find_all('cl', recursive=False):
        # å¤åˆ¶èŠ‚ç‚¹ä»¥é¿å…ä¿®æ”¹åŸå§‹ soup
        cl_copy = cl.__copy__()
        # æå–ä¸­æ–‡
        chn = cl.find('chn')
        if chn:
            chn_text = chn.get_text(strip=True)
        # æå–è‹±æ–‡æ­é…è¯ï¼ˆå»æ‰ chn å’Œ chnsepï¼‰
        for tag in cl.find_all(['chn', 'chnsep']):
            tag.decompose()
        cl_text = cl.get_text(strip=True)
        if cl_text:
            collocations.append(cl_text)

    if not collocations:
        return None

    # æå–ä¾‹å¥
    examples = []
    for x_blk in sb_g.find_all('x-blk', recursive=False):
        x_elem = x_blk.find('x')
        if x_elem:
            x_chn = x_elem.find('chn')
            ex_cn = x_chn.get_text(strip=True) if x_chn else ""
            # å»æ‰ä¸­æ–‡è·å–è‹±æ–‡ä¾‹å¥
            for tag in x_elem.find_all(['chn', 'chnsep', 'fthzmark']):
                tag.decompose()
            ex_en = x_elem.get_text(strip=True)
            if ex_en:
                examples.append({'en': ex_en, 'cn': ex_cn})

    return {
        'words': collocations,
        'chn': chn_text,
        'examples': examples,
    }

# ================== æ­¥éª¤4: ç”Ÿæˆ Anki å¡ç‰‡å­—æ®µ ==================

def generate_collocations_html(card):
    """ç”Ÿæˆæ­é…å†…å®¹ HTMLï¼ˆåŒ…å«ä¸­è‹±æ–‡ï¼Œç”±æ¨¡æ¿ CSS æ§åˆ¶æ˜¾éšï¼‰"""
    groups_html = ""
    for group in card['collocation_groups']:
        groups_html += f'<div class="colloc-group">'
        groups_html += f'<div class="colloc-category">{group["category"]}</div>'

        for item in group['items']:
            groups_html += '<div class="colloc-item">'
            words_str = ' <span class="sep">|</span> '.join(
                f'<span class="colloc-word">{w}</span>' for w in item['words']
            )
            if item['chn']:
                words_str += f'<span class="colloc-chn">{item["chn"]}</span>'
            groups_html += f'<div class="colloc-words">{words_str}</div>'

            for ex in item['examples']:
                groups_html += '<div class="colloc-example">'
                groups_html += f'<div class="ex-en">âœ¦ {ex["en"]}</div>'
                if ex['cn']:
                    groups_html += f'<div class="ex-cn">{ex["cn"]}</div>'
                groups_html += '</div>'

            groups_html += '</div>'
        groups_html += '</div>'

    return groups_html


def generate_anki_import_file(all_cards):
    """ç”Ÿæˆ Anki å¯¼å…¥æ–‡ä»¶ (TSV)
    æ ¼å¼: Word<tab>POS<tab>SenseNum<tab>DefEN<tab>DefCN<tab>Collocations<tab>FreqRank<tab>Tags
    """
    lines = []
    for card in all_cards:
        colloc = generate_collocations_html(card).replace('\n', '').replace('\r', '')
        freq_rank = str(card.get('freq_rank', ''))
        fields = [
            card['word'],
            card['pos'],
            card['sense_num'],
            card['def_en'],
            card['def_cn'],
            colloc,
            freq_rank,
            card['word'],  # tag
        ]
        lines.append('\t'.join(fields))
    return '\n'.join(lines)

# ================== CSS æ ·å¼ ==================

CARD_CSS = '''/* Anki æ­é…å¡ç‰‡æ ·å¼ */

.card {
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Arial, sans-serif;
  font-size: 16px;
  text-align: left;
  background: #f5f5f5;
  padding: 20px;
}

.colloc-card {
  max-width: 600px;
  margin: 0 auto;
  background: #fff;
  padding: 28px;
  border-radius: 12px;
  box-shadow: 0 2px 12px rgba(0,0,0,0.08);
}

/* å•è¯ */
.word {
  font-size: 36px;
  font-weight: bold;
  color: #2c3e50;
  margin-bottom: 6px;
}

/* è¯æ€§ + ä¹‰é¡¹ç¼–å· */
.meta {
  margin-bottom: 12px;
}

.pos {
  color: #9b59b6;
  font-style: italic;
  font-size: 16px;
  font-weight: 600;
  margin-right: 10px;
}

.sense-num {
  display: inline-block;
  background: #e74c3c;
  color: white;
  padding: 2px 10px;
  border-radius: 12px;
  font-weight: bold;
  font-size: 14px;
}

.freq-rank {
  font-size: 14px;
  color: #95a5a6;
  font-weight: normal;
  margin-left: 8px;
}

/* é‡Šä¹‰ */
.definition {
  padding: 12px 16px;
  background: #f8f9fa;
  border-radius: 8px;
  border-left: 4px solid #3498db;
  margin-bottom: 8px;
}

.def-en {
  color: #2c3e50;
  font-size: 18px;
  font-weight: 500;
}

.def-cn {
  color: #7f8c8d;
  font-size: 16px;
  margin-left: 8px;
}

/* åˆ†å‰²çº¿ */
.divider {
  margin: 20px 0;
  border: none;
  border-top: 2px solid #ecf0f1;
}

/* æ­é…ç»„ */
.colloc-group {
  margin-bottom: 20px;
}

.colloc-category {
  font-size: 13px;
  font-weight: 700;
  color: #e67e22;
  text-transform: uppercase;
  letter-spacing: 1px;
  padding: 4px 10px;
  background: #fef5e7;
  border-radius: 4px;
  margin-bottom: 10px;
  display: inline-block;
}

/* æ­é…æ¡ç›® */
.colloc-item {
  margin-bottom: 12px;
  padding-left: 8px;
}

.colloc-words {
  margin-bottom: 4px;
  line-height: 1.8;
}

.colloc-word {
  color: #2980b9;
  font-weight: 600;
  font-size: 16px;
}

.sep {
  color: #bdc3c7;
  margin: 0 4px;
}

.colloc-chn {
  color: #7f8c8d;
  font-size: 14px;
  margin-left: 8px;
}

/* ä¾‹å¥ */
.colloc-example {
  padding: 6px 12px;
  margin: 4px 0 4px 12px;
  border-left: 2px solid #e0e0e0;
}

.ex-en {
  color: #555;
  font-style: italic;
  font-size: 15px;
  line-height: 1.6;
}

.ex-cn {
  color: #95a5a6;
  font-size: 14px;
  line-height: 1.5;
  margin-left: 16px;
}

/* æ­£é¢éšè—ä¸­æ–‡ */
.hide-cn .def-cn,
.hide-cn .colloc-chn,
.hide-cn .ex-cn {
  display: none;
}
'''

CARD_TEMPLATE_FRONT = '''<div class="colloc-card hide-cn">
  <div class="word">{{Word}}{{#FreqRank}}<span class="freq-rank">#{{FreqRank}}</span>{{/FreqRank}}</div>
  <div class="meta">
    <span class="pos">{{POS}}</span>
    {{#SenseNum}}<span class="sense-num">#{{SenseNum}}</span>{{/SenseNum}}
  </div>
  {{#DefEN}}
  <div class="definition">
    <span class="def-en">{{DefEN}}</span>
    <span class="def-cn">{{DefCN}}</span>
  </div>
  {{/DefEN}}
  <hr class="divider">
  <div class="colloc-content">{{Collocations}}</div>
</div>'''

CARD_TEMPLATE_BACK = '''<div class="colloc-card">
  <div class="word">{{Word}}{{#FreqRank}}<span class="freq-rank">#{{FreqRank}}</span>{{/FreqRank}}</div>
  <div class="meta">
    <span class="pos">{{POS}}</span>
    {{#SenseNum}}<span class="sense-num">#{{SenseNum}}</span>{{/SenseNum}}
  </div>
  {{#DefEN}}
  <div class="definition">
    <span class="def-en">{{DefEN}}</span>
    <span class="def-cn">{{DefCN}}</span>
  </div>
  {{/DefEN}}
  <hr class="divider">
  <div class="colloc-content">{{Collocations}}</div>
</div>'''

# ================== ä¸»ç¨‹åº ==================

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='Anki æ­é…å¡ç‰‡ç”Ÿæˆå™¨ - ä»ç‰›æ´¥æ­é…è¯å…¸ç”Ÿæˆæ­é…å¡ç‰‡',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ç¤ºä¾‹:
  å¤„ç†æ•´ä¸ªè¯å…¸:
    python collocation_generator.py --all

  ä»å‘½ä»¤è¡Œå•è¯åˆ—è¡¨:
    python collocation_generator.py -w pitch formidable accord

  ä»æ–‡ä»¶è¯»å–:
    python collocation_generator.py -f words.txt

  ä» Anki æ•°æ®åº“è¯»å–:
    python collocation_generator.py -a collection.anki2
        '''
    )
    parser.add_argument('-w', '--words', nargs='+', metavar='WORD',
                        help='å•è¯åˆ—è¡¨ï¼ˆç©ºæ ¼åˆ†éš”ï¼‰')
    parser.add_argument('-f', '--file', metavar='FILE',
                        help='åŒ…å«å•è¯åˆ—è¡¨çš„æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªå•è¯ï¼‰')
    parser.add_argument('-a', '--anki', metavar='DB',
                        help='ä» Anki æ•°æ®åº“æå–éš¾è¯')
    parser.add_argument('--all', action='store_true',
                        help='å¤„ç†æ•´ä¸ªè¯å…¸çš„æ‰€æœ‰å•è¯')
    parser.add_argument('--mdx-dir', metavar='DIR',
                        help=f'MDX è¯å…¸ç›®å½•ï¼ˆé»˜è®¤: {MDX_DICT_DIR}ï¼‰')
    parser.add_argument('--max', type=int, default=0,
                        help='æœ€å¤šå¤„ç†çš„å•è¯æ•°ï¼ˆ0 = ä¸é™åˆ¶ï¼‰')
    parser.add_argument('--freq', metavar='FILE',
                        help=f'è¯é¢‘å­—å…¸æ–‡ä»¶ï¼ˆé»˜è®¤: {FREQ_DICT_FILE}ï¼‰')
    return parser.parse_args()


def main():
    print("=" * 60)
    print("  Anki æ­é…å¡ç‰‡ç”Ÿæˆå™¨ (Oxford Collocation Dictionary)")
    print("=" * 60)
    print()

    args = parse_arguments()

    # æ¸…ç©ºæ—¥å¿—
    open(SKIPPED_LOG, 'w').close()

    # æ£€æŸ¥æŸ¥è¯¢æ–¹å¼
    use_direct = args.all  # --all æ¨¡å¼è‡ªåŠ¨ä½¿ç”¨ç›´æ¥æŸ¥è¯¢
    mdx_dir = args.mdx_dir

    if use_direct:
        print("ğŸ” åˆå§‹åŒ–ç›´æ¥è¯å…¸æŸ¥è¯¢...")
        builder = _get_mdx_builder(mdx_dir)
        if builder is None:
            print("âŒ æ— æ³•åŠ è½½ MDX è¯å…¸æ–‡ä»¶")
            return
        print("âœ… è¯å…¸åŠ è½½å®Œæˆ\n")
    else:
        print("ğŸ” æ£€æŸ¥ MDX-Server è¿æ¥...")
        if not check_mdx_server():
            print(f"âŒ æ— æ³•è¿æ¥åˆ° MDX-Server: {MDX_SERVER_URL}")
            print()
            print("è¯·å…ˆå¯åŠ¨ mdx-server:")
            print('  cd ../anki-vocab-builder/mdx-server')
            print('  python mdx_server.py "../../ç‰›æ´¥è‹±è¯­æ­é…è¯å…¸å…¨ç´¢å¼•/"')
            return
        print(f"âœ… MDX-Server è¿è¡Œæ­£å¸¸\n")

    # åŠ è½½è¯é¢‘å­—å…¸
    freq_file = args.freq or FREQ_DICT_FILE
    freq_map = load_freq_dict(freq_file)

    # è·å–å•è¯åˆ—è¡¨
    if args.all:
        print("ğŸ“š ä»è¯å…¸æå–æ‰€æœ‰è¯å¤´...")
        word_list = get_all_dictionary_words(mdx_dir)
    elif args.words:
        print(f"ğŸ“š ä»å‘½ä»¤è¡Œå‚æ•°è¯»å–å•è¯ ({len(args.words)} ä¸ª)...")
        word_list = get_words_from_list(args.words)
    elif args.file:
        print(f"ğŸ“š ä»æ–‡ä»¶è¯»å–å•è¯: {args.file}...")
        word_list = get_words_from_file(args.file)
    elif args.anki:
        print(f"ğŸ“š ä» Anki æ•°æ®åº“æå–éš¾è¯: {args.anki}...")
        global ANKI_DB
        ANKI_DB = args.anki
        word_list = get_difficult_words()
    else:
        print("âŒ è¯·æŒ‡å®šå•è¯æ¥æº: --all, -w, -f, æˆ– -a")
        print("  ç”¨ --help æŸ¥çœ‹å¸®åŠ©")
        return

    if not word_list:
        print("âŒ æœªæ‰¾åˆ°å•è¯")
        return

    if args.max > 0 and len(word_list) > args.max:
        print(f"âš ï¸  å•è¯æ•°é‡è¶…è¿‡é™åˆ¶ï¼Œå°†åªå¤„ç†å‰ {args.max} ä¸ª")
        word_list = word_list[:args.max]

    # å»é‡
    seen = set()
    unique_words = []
    for w in word_list:
        if w['word'] not in seen:
            seen.add(w['word'])
            unique_words.append(w)
    word_list = unique_words

    print(f"âœ… æ‰¾åˆ° {len(word_list)} ä¸ªå•è¯\n")

    # æŸ¥è¯¢å¹¶è§£æ
    print("ğŸ” æŸ¥è¯¢è¯å…¸å¹¶è§£ææ­é…...\n")
    all_cards = []
    failed_words = []
    total = len(word_list)
    success_count = 0
    # å¤§æ‰¹é‡æ—¶å‡å°‘è¾“å‡º
    verbose = total <= 50

    for i, item in enumerate(word_list, 1):
        word = item['word']

        if verbose:
            print(f"[{i}/{total}] {word:20}", end=" ", flush=True)
        elif i % 200 == 0 or i == total:
            print(f"  è¿›åº¦: {i}/{total} ({i*100//total}%)  å¡ç‰‡: {len(all_cards)}  æˆåŠŸ: {success_count}", flush=True)

        # æŸ¥è¯¢
        if use_direct:
            html = query_mdx_direct(word, mdx_dir)
        else:
            html = query_mdx_server(word)

        if html:
            cards = parse_collocation_html(html, word)
            if cards:
                # é™„åŠ è¯é¢‘åºå·
                rank = freq_map.get(word.lower(), '')
                for card in cards:
                    card['freq_rank'] = str(rank)
                all_cards.extend(cards)
                success_count += 1
                if verbose:
                    print(f"â†’ {len(cards)} å¼ å¡ç‰‡ âœ“")
            else:
                if verbose:
                    print("â†’ æ— åŠ¨è¯/ä»‹è¯æ­é… âœ—")
                logger.info(f"SKIP {word}: æ— åŠ¨è¯/ä»‹è¯æ­é…")
                failed_words.append(word)
        else:
            if verbose:
                print("â†’ æŸ¥è¯¢å¤±è´¥ âœ—")
            logger.info(f"SKIP {word}: æŸ¥è¯¢å¤±è´¥")
            failed_words.append(word)

    print()

    if not all_cards:
        print("âŒ æœªç”Ÿæˆä»»ä½•å¡ç‰‡")
        return

    # ç”Ÿæˆå¯¼å…¥æ–‡ä»¶
    print("ğŸ“ ç”Ÿæˆ Anki å¯¼å…¥æ–‡ä»¶...")
    import_content = generate_anki_import_file(all_cards)

    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write(import_content)

    # ä¿å­˜æ ·å¼
    css_file = "anki_card_style.css"
    with open(css_file, 'w', encoding='utf-8') as f:
        f.write(CARD_CSS)

    # ä¿å­˜æ¨¡æ¿
    template_file = "anki_card_template.txt"
    with open(template_file, 'w', encoding='utf-8') as f:
        f.write("=== æ­£é¢æ¨¡æ¿ ===\n")
        f.write(CARD_TEMPLATE_FRONT)
        f.write("\n\n=== èƒŒé¢æ¨¡æ¿ ===\n")
        f.write(CARD_TEMPLATE_BACK)
        f.write("\n\n=== æ ·å¼(CSS) ===\n")
        f.write(CARD_CSS)

    # å®Œæˆ
    print()
    print("=" * 60)
    print("âœ… å®Œæˆ!")
    print(f"  ç”Ÿæˆå¡ç‰‡: {len(all_cards)} å¼ ")
    print(f"  æ¥è‡ªå•è¯: {len(word_list) - len(failed_words)} / {len(word_list)}")
    print(f"  å¯¼å…¥æ–‡ä»¶: {OUTPUT_FILE}")
    print(f"  æ ·å¼æ–‡ä»¶: {css_file}")
    print(f"  æ¨¡æ¿æ–‡ä»¶: {template_file}")

    if failed_words:
        print(f"\nâš ï¸  è·³è¿‡çš„å•è¯ ({len(failed_words)} ä¸ª):")
        print(f"  {', '.join(failed_words[:20])}")
        if len(failed_words) > 20:
            print(f"  ... è¿˜æœ‰ {len(failed_words) - 20} ä¸ª")
        print(f"  è¯¦è§æ—¥å¿—: {SKIPPED_LOG}")

    print()
    print("ğŸ“Œ å¯¼å…¥æ­¥éª¤:")
    print("  1. åœ¨ Anki ä¸­: å·¥å…· â†’ ç®¡ç†ç¬”è®°ç±»å‹ â†’ æ·»åŠ ")
    print("  2. é€‰æ‹©ã€ŒåŸºç¡€ã€ï¼Œå‘½åä¸ºã€Œæ­é…å¡ç‰‡ã€")
    print("  3. å­—æ®µ: æ·»åŠ  Word, POS, SenseNum, DefEN, DefCN, Collocations, FreqRank")
    print("     ï¼ˆåˆ é™¤é»˜è®¤çš„ Front/Backï¼‰")
    print("  4. ç‚¹å‡»ã€Œå¡ç‰‡ã€ï¼Œå¤åˆ¶ anki_card_template.txt ä¸­çš„:")
    print("     - æ­£é¢æ¨¡æ¿ â†’ ç²˜è´´åˆ°ã€Œæ­£é¢æ¨¡æ¿ã€")
    print("     - èƒŒé¢æ¨¡æ¿ â†’ ç²˜è´´åˆ°ã€ŒèƒŒé¢æ¨¡æ¿ã€")
    print("     - æ ·å¼ â†’ ç²˜è´´åˆ°ã€Œæ ·å¼ã€")
    print(f"  5. æ–‡ä»¶ â†’ å¯¼å…¥ï¼Œé€‰æ‹© {OUTPUT_FILE}")
    print("  6. ç±»å‹é€‰æ‹©ã€Œæ­é…å¡ç‰‡ã€ï¼Œåˆ†éš”ç¬¦: Tabï¼Œå…è®¸HTML")
    print("  7. å­—æ®µæ˜ å°„: Word, POS, SenseNum, DefEN, DefCN, Collocations, FreqRank, æ ‡ç­¾")


if __name__ == "__main__":
    main()
